{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "model = gensim.models.Word2Vec.load('spamDetect_20180217.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "äºŒå¤© 0.962252557278\n",
      "åˆšä¹° 0.961527764797\n",
      "å¤Ÿä¸ç€ 0.961062729359\n",
      "æ²¡äº‹ 0.956673383713\n",
      "xinjiang 0.955379366875\n",
      "å¶ 0.955097436905\n",
      "è¿½ 0.954735636711\n",
      "å»ç¨‹ 0.954279601574\n",
      "æ€€é‡Œ 0.95322817564\n",
      "ç•™åœ¨ 0.95120036602\n",
      "<type 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(u'èŠ±')\n",
    "for word in result:\n",
    "    print word[0],word[1]\n",
    "print(type(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8019 lines...\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "7989    1\n",
      "7990    1\n",
      "7991    1\n",
      "7992    1\n",
      "7993    1\n",
      "7994    1\n",
      "7995    1\n",
      "7996    1\n",
      "7997    1\n",
      "7998    1\n",
      "7999    1\n",
      "8000    1\n",
      "8001    1\n",
      "8002    1\n",
      "8003    1\n",
      "8004    1\n",
      "8005    1\n",
      "8006    1\n",
      "8007    1\n",
      "8008    1\n",
      "8009    1\n",
      "8010    1\n",
      "8011    1\n",
      "8012    1\n",
      "8013    1\n",
      "8014    1\n",
      "8015    1\n",
      "8016    1\n",
      "8017    1\n",
      "8018    1\n",
      "Name: flag, Length: 8019, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# train = pd.read_csv( \"shuf_spam_train.csv\", header=0, delimiter=\",\", quoting=2)\n",
    "train = pd.read_csv( \"spam_train_5_5.csv\")\n",
    "print(\"Read %d lines...\" % (train[\"sentence\"].shape))\n",
    "\n",
    "train_data = train[\"sentence\"]\n",
    "train_label = train[\"flag\"]\n",
    "\n",
    "# print(train_data[0])\n",
    "print(train_label)\n",
    "# print(train_label)\n",
    "# print(train_data)\n",
    "# for i in range(len(train)):\n",
    "#     if(train_label[i] == 0.):\n",
    "#         print(train_data[i])\n",
    "        \n",
    "# print(train_data[20077])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jieba\n",
    "# import io\n",
    "\n",
    "# stopwords = [line.strip() for line in io.open('stopwords.txt', 'r', encoding='utf-8').readlines()] \n",
    "# stopwords = set(stopwords)\n",
    "\n",
    "# for i in range(len(neg_sentences)):\n",
    "#     line_seg = jieba.cut(neg_sentences[i].strip())\n",
    "#     outstr = \"\"\n",
    "#     for word in line_seg:  \n",
    "#         if word not in stopwords:\n",
    "#             outstr += word  \n",
    "#             outstr += \" \"\n",
    "        \n",
    "#     neg_sentences[i] = outstr.strip()\n",
    "#     outstr = \"\"\n",
    "#     if(i % 10000 == 0):\n",
    "#         print(\"neg deal %s of %s\" % (i,len(neg_sentences)))\n",
    "        \n",
    "# for i in range(len(pos_sentences)):\n",
    "#     line_seg = jieba.cut(pos_sentences[i].strip())\n",
    "#     outstr = \"\"\n",
    "#     for word in line_seg:  \n",
    "#         if word not in stopwords:\n",
    "#             outstr += word  \n",
    "#             outstr += \" \"\n",
    "        \n",
    "#     pos_sentences[i] = outstr.strip()\n",
    "#     outstr = \"\"\n",
    "#     if(i % 10000 == 0):\n",
    "#         print(\"pos deal %s of %s\" % (i,len(pos_sentences)))\n",
    "\n",
    "# del stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 324\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "\n",
    "    nwords = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for word in words:\n",
    "        if word.decode(\"utf-8\") in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word.decode(\"utf-8\")])\n",
    "    if(nwords > 0):\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(sentences, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    print(len(sentences))\n",
    "    reviewFeatureVecs = np.zeros((len(sentences),num_features),dtype=\"float32\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "       if(isinstance(sentence,basestring)==False):\n",
    "            sentence=\" \"\n",
    "       if counter%1000 == 0:\n",
    "           print \"Review %d of %d\" % (counter, len(sentences))\n",
    "       words = sentence.split(\" \")\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(words, model, num_features)\n",
    "\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "å›å¤ éŸµ lt ç‰ˆæƒ ç½‘æ˜“ äº‘ éŸ³ä¹ æ‰¾\n",
      "8019\n",
      "Review 0 of 8019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 8019\n",
      "Review 2000 of 8019\n",
      "Review 3000 of 8019\n",
      "Review 4000 of 8019\n",
      "Review 5000 of 8019\n",
      "Review 6000 of 8019\n",
      "Review 7000 of 8019\n",
      "Review 8000 of 8019\n",
      "<type 'numpy.ndarray'>\n",
      "8019\n",
      "[[-0.05196826  0.62578809  0.33745331 ...,  0.07365704 -0.18128303\n",
      "   0.05877255]\n",
      " [-0.13448544 -0.15526298  0.27640295 ..., -0.28622359 -0.1406382\n",
      "  -0.25426164]\n",
      " [-0.04657002  0.63925362 -0.04474411 ..., -0.41634336 -0.48093891\n",
      "   0.03007146]\n",
      " ..., \n",
      " [ 0.04947954  0.23000774  0.35520238 ...,  0.10404185 -0.23208822\n",
      "   0.19359264]\n",
      " [ 0.01632851  0.13351597  0.69430727 ...,  0.09240357 -0.31910026\n",
      "   0.12541388]\n",
      " [-0.04290846  0.18934393  0.69835955 ...,  0.14514384 -0.36947876\n",
      "   0.15277256]]\n"
     ]
    }
   ],
   "source": [
    "train_clean_data = []\n",
    "for review in train[\"sentence\"]:\n",
    "    train_clean_data.append(review)\n",
    "print(type(train_clean_data)) \n",
    "print(train_clean_data[0])\n",
    "train_clean_data = getAvgFeatureVecs(train_clean_data, model, num_features )\n",
    "print(type(train_clean_data))\n",
    "print(len(train_clean_data))\n",
    "print(train_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_weight_variable(layer_name, shape):\n",
    "    \"\"\" Retrieve an existing variable with the given layer name \n",
    "    \"\"\"\n",
    "    return tf.get_variable(layer_name, shape=shape, initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "def fc_weight_variable(layer_name, shape):\n",
    "    \"\"\" Retrieve an existing variable with the given layer name\n",
    "    \"\"\"\n",
    "    return tf.get_variable(layer_name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\" Creates a new bias variable\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.constant(0.0, shape=shape))\n",
    "\n",
    "#å·ç§¯å±‚\n",
    "def conv_layer(input,               # å‰ä¸€å±‚çš„è¾“å‡º\n",
    "                layer_name,         # å½“å‰å±‚çš„åå­—\n",
    "                num_input_channels, # å‰ä¸€å±‚çš„ç¥ç»å…ƒä¸ªæ•°\n",
    "                filter_size,        # æ»¤æ³¢å™¨é•¿åº¦ï¼å®½åº¦\n",
    "                num_filters,        # æ»¤æ³¢å™¨ä¸ªæ•°\n",
    "                pooling=True):      # æ˜¯å¦ä½¿ç”¨2X2çš„æ± åŒ–å±‚\n",
    "\n",
    "    # å·ç§¯å±‚çš„æƒé‡å±æ€§\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # åˆ›å»ºå½“å‰å±‚çš„æƒé‡\n",
    "    weights = conv_weight_variable(layer_name, shape=shape)\n",
    "    \n",
    "    # æ„é€ åç§»é‡\n",
    "    biases = bias_variable(shape=[num_filters])\n",
    "\n",
    "    # åˆ›å»ºä¸€ä¸ªæ»¤æ³¢å™¨\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME') # with zero padding\n",
    "\n",
    "    layer += biases\n",
    "    \n",
    "    # ä½¿ç”¨RELUæ¿€æ´»å‡½æ•°\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # æ˜¯å¦ä½¿ç”¨æ± åŒ–å±‚ï¼Œè¿™é‡Œä½¿ç”¨maxpool\n",
    "    if pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "#é™ä½æ•°æ®ç»´åº¦åˆ°ä¸€ç»´\n",
    "def flatten_layer(layer):\n",
    "    # è·å–å±‚ç»“æ„å±æ€§\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # è®¡ç®—å±‚çš„featuresæ•°é‡: img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # é‡æ–°å°†å±‚æ„é€ æˆ [å›¾åƒä¸ªæ•°, featuresæ•°é‡].\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer_flat, num_features\n",
    "\n",
    "\n",
    "#å…¨é“¾æ¥å±‚\n",
    "def fc_layer(input,        # å‰ä¸€å±‚è¾“å‡º\n",
    "             layer_name,   # å½“å‰å±‚çš„åå­—\n",
    "             num_inputs,   # å‰ä¸€å±‚çš„è¾“å…¥ä¸ªæ•°\n",
    "             num_outputs,  # è¾“å‡ºä¸ªæ•°\n",
    "             relu=True):   # æ˜¯å¦ä½¿ç”¨RELUæ¿€æ´»å‡½æ•°\n",
    "\n",
    "    # æ„é€ æ»¤æ³¢å™¨å’Œåç§»é‡\n",
    "    weights = fc_weight_variable(layer_name, shape=[num_inputs, num_outputs])\n",
    "    biases = bias_variable(shape=[num_outputs])\n",
    "\n",
    "    # è®¡ç®—\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # æ˜¯å¦ä½¿ç”¨RELUæ¿€æ´»å‡½æ•°\n",
    "    if relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "    return layer\n",
    "\n",
    "# ç¬¬ä¸€å±‚å·ç§¯å±‚\n",
    "filter_size1 = 4         # æ»¤æ³¢å™¨5*5\n",
    "num_filters1 = 32         # éšè—ç¥ç»å…ƒä¸ªæ•°\n",
    "\n",
    "# ç¬¬äºŒå±‚å·ç§¯å±‚\n",
    "filter_size2 = 4          # æ»¤æ³¢å™¨5*5\n",
    "num_filters2 = 64         # éšè—ç¥ç»å…ƒä¸ªæ•°\n",
    "\n",
    "# å…¨é“¾æ¥å±‚ç¥ç»å…ƒä¸ªæ•°\n",
    "fc_size = 128\n",
    "\n",
    "num_channels = 1\n",
    "width_v = 18\n",
    "long_v = 18\n",
    "num_classes = 2\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, width_v, long_v, num_channels), name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "#y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "y_true_cls = tf.argmax(y_true, 1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#è®¡ç®—ç¬¬ä¸€ä¸ªå·ç§¯å±‚\n",
    "conv_1, w_c1 = conv_layer(input=x,\n",
    "                          layer_name=\"conv_1\",\n",
    "                          num_input_channels=num_channels,\n",
    "                          filter_size=filter_size1,\n",
    "                          num_filters=num_filters1, pooling=True)\n",
    "\n",
    "conv_1\n",
    "\n",
    "#è®¡ç®—ç¬¬äºŒä¸ªå·ç§¯å±‚\n",
    "conv_2, w_c2 = conv_layer(input=conv_1,\n",
    "                          layer_name=\"conv_2\",\n",
    "                          num_input_channels=num_filters1,\n",
    "                          filter_size=filter_size2,\n",
    "                          num_filters=num_filters2,\n",
    "                          pooling=True)\n",
    "\n",
    "# ä½¿ç”¨dropoutå±‚ï¼Œé¿å…è¿‡æ‹Ÿåˆ\n",
    "dropout = tf.nn.dropout(conv_2, keep_prob)\n",
    "\n",
    "dropout\n",
    "\n",
    "#é™ä½åˆ°ä¸€ç»´æ•°æ®\n",
    "layer_flat, num_features = flatten_layer(dropout)\n",
    "\n",
    "layer_flat\n",
    "\n",
    "#å…¨é“¾æ¥å±‚\n",
    "fc_1 = fc_layer(input=layer_flat,\n",
    "                layer_name=\"fc_1\",\n",
    "                num_inputs=num_features,\n",
    "                num_outputs=fc_size,\n",
    "                relu=True)\n",
    "\n",
    "fc_1\n",
    "\n",
    "fc_2 = fc_layer(input=fc_1,\n",
    "                layer_name=\"fc_2\",\n",
    "                num_inputs=fc_size,\n",
    "                num_outputs=num_classes,\n",
    "                relu=False)\n",
    "\n",
    "fc_2\n",
    "\n",
    "#ä½¿ç”¨softmax\n",
    "y_pred = tf.nn.softmax(fc_2)\n",
    "\n",
    "# The class-number is the index of the largest element.\n",
    "#y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "y_pred_cls = tf.argmax(y_pred, 1)\n",
    "\n",
    "# Calcualte the cross-entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc_2, labels=y_true)\n",
    "\n",
    "# Take the average of the cross-entropy for all the image classifications.\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Global step is required to compute the decayed learning rate\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "# Apply exponential decay to the learning rate\n",
    "learning_rate = tf.train.exponential_decay(0.001, global_step, 10000, 0.96, staircase=True)\n",
    "\n",
    "# ä½¿ç”¨AdagradOptimizerè‡ªé€‚åº”å­¦ä¹ ç‡å’ŒBPæ›´æ–°æƒé‡\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "\n",
    "# è®¡ç®—è¯¯å·®\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "\n",
    "# ç”¨æ–¹å·®è¡¨ç¤ºè¯¯å·®\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# æ¯ä¸€æ‰¹è®­ç»ƒæ ·æœ¬çš„ä¸ªæ•° \n",
    "batch_size = 128\n",
    "\n",
    "# dropoutæ¯”ä¾‹\n",
    "dropout = 0.5\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "label = train[\"flag\"]\n",
    "\n",
    "#è®­ç»ƒå‡½æ•°\n",
    "def optimize(num_iterations, display_step):\n",
    "    #å¤„ç†æ ‡ç­¾æ•°æ®\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    "    \n",
    "#     for i in range(len(label)):\n",
    "#         label[i] = int(label[i])\n",
    "#         if i%1000 == 0:\n",
    "#            print \"deal %d of %d\" % (i, len(label))\n",
    "        \n",
    "    train_data = train_clean_data[0 : int(len(train_clean_data) * 1),:]\n",
    "    train_label = label[0 : int(len(train_clean_data) * 1)]\n",
    "    \n",
    "#     for l in train_label:\n",
    "#         print(l)\n",
    "    \n",
    "    # ä½¿ç”¨ont-hotç¼–ç \n",
    "    enc = OneHotEncoder().fit(train_label.reshape(-1, 1))\n",
    "    train_label = enc.transform(train_label.reshape(-1, 1)).toarray()\n",
    "    \n",
    "#     print(train_label)\n",
    "    \n",
    "    global total_iterations\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(0,num_iterations):\n",
    "\n",
    "        offset = (step * batch_size) % (int(len(train_clean_data) * 1) - batch_size)\n",
    "        \n",
    "        batch_x = train_clean_data[offset:(offset + batch_size),:]\n",
    "        batch_y = train_label[offset:(offset + batch_size),:]\n",
    "        \n",
    "        batch_x = batch_x.reshape(-1,width_v,long_v,num_channels)\n",
    "        \n",
    "        feed_dict_train = {x: batch_x, y_true: batch_y, keep_prob: dropout}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "#         print(\"12312312313,\",batch_x)\n",
    "        if step % display_step == 0:\n",
    "#             pre = session.run(y_pred_cls, {x:batch_x , y_true:batch_y , keep_prob: 1.0})\n",
    "#             softmax = session.run(y_pred, {x:batch_x , y_true:batch_y , keep_prob: 1.0})\n",
    "#             print(\"softmax:\",softmax)\n",
    "#             print(\"pre:\",pre)\n",
    "#             print(\"true:\",batch_y)\n",
    "            \n",
    "            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            print(\"Minibatch accuracy at step %d: %.4f\" % (step, batch_acc))\n",
    "            \n",
    "            \n",
    "#             validation_acc = session.run(accuracy, {x:test_data , y_true:test_label , keep_prob: 1.0})\n",
    "#             print(\"Validation accuracy: %.4f\" % validation_acc)\n",
    "\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    time_diff = time.time() - start_time\n",
    "    \n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_diff)))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore the model...\n",
      "INFO:tensorflow:Restoring parameters from /notebooks/SpamTrain/model_cnn_256_06/comment_cnn_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "if(os.path.exists(\"/notebooks/SpamTrain/model_cnn_256_06/checkpoint\") == False):\n",
    "    # increase num_iterations to get better accuracy = 5000 to 50000 &  display_step=500\n",
    "    optimize(num_iterations=(len(train_clean_data)-100), display_step=100)\n",
    "    saver_path = saver.save(session, \"/notebooks/SpamTrain/model_cnn_256_06/comment_cnn_model.ckpt\")  # ä¿å­˜æ¨¡å‹\n",
    "else:\n",
    "   print(\"restore the model...\")\n",
    "   saver.restore(session,'/notebooks/SpamTrain/model_cnn_256_06/comment_cnn_model.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8790\n",
      "Validation accuracy: 0.8790\n",
      "Validation accuracy: 0.8790\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    batch_x = train_clean_data[i * 1000: (i+1) * 1000,:]\n",
    "    batch_x = batch_x.reshape(-1,width_v,long_v,num_channels)\n",
    "    train_label = train[\"flag\"]\n",
    "    # print(train_label)\n",
    "    enc = OneHotEncoder().fit(train_label.reshape(-1, 1))\n",
    "    train_label = enc.transform(train_label.reshape(-1, 1)).toarray()\n",
    "    train_label = train_label[i * 1000: (i+1) * 1000]\n",
    "    # print(train_label)\n",
    "\n",
    "    validation_acc = session.run(accuracy, {x:batch_x , y_true:train_label , keep_prob: 1.0})\n",
    "    pre = session.run(y_pred_cls, {x:batch_x, keep_prob: 1.0})\n",
    "    print(\"Validation accuracy: %.4f\" % validation_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»¿èŒ¶ å©Š\n",
      "ä¸»æ’­ å–œæ¬¢ ç‹— ç®¡ç† ç¿»ä¸èµ· æµªæ¥\n",
      "åŠæ¯› é“å£« ç»“ç»“å·´å·´ å‚» é€¼ æ ·å­\n",
      "å®‰çœ è¯\n",
      "åŠ ä¸ª å¾®ä¿¡ ç¾å¥³\n",
      "å¥³å¨² è‹è€ å¸ˆ æ—è¾¹ è·³èˆ å¸® æˆ‘åŠ© åŠ©å¨ mc å¤©ä½‘ å¹ç‰› å¹ç‰› å›½ç²¹ æ‰¯ å…³ç³» ä¸­å›½ å›½ç²¹ ä½å»‰\n",
      "å®‹åº†é¾„ ä¿¡å®ˆè¯ºè¨€\n",
      "åŠ  å¾®ä¿¡ ç§æˆ¿ç…§ nbsp nbsp æŒ– é¼»å±\n",
      "çœŸ äººç§° ä¸ç”¨ å¾®ä¿¡ è‹ç™½ è°è¨€ æ— åŠ› zhuzhang æƒåŠ› æ€€å¿µ QQ æ„Ÿè§‰ QQ æµæ°“\n",
      "ä½ å¥½ å†’æ˜§ æ‰“æ‰° åš æ­£å“ å·¦ å“¼å“¼ Vans é˜´é™© ä¸‡æ–¯ è€å…‹é‹ ä¸“æŸœ ä»£ è´­é˜¿è¿ª æ„¤æ€’ NB åŒ¡å¨ æ­£å“ ä½“è‚²è¿åŠ¨ é‹æœ ä»·æ ¼ ä¸“ æ‡’å¾— ç† æŸœ å®ä½“åº— è¯´ ç™½èœ å°åº— åš è‰¯å¿ƒ æ­£å“ æ”¯æŒ å½¢å¼ é‰´å®š äº¤æ˜“ æ™• ç»ˆèº« æœ‹å‹ æœ‰æ„ V ğŸ’˜ Vans8765 æœ‹å‹åœˆ ğŸˆ¶ æ›´æ–° æ‰“æ‰° è¯· æµ·æ¶µ ğŸ™\n",
      "æ¸©æƒ…è„‰è„‰ momo å››å£° åšé¬¼è„¸\n",
      "Assen æ·å”±è¿‡ é£æ ¼ æ›²å­ ç§»æ­¥ ç½‘æ˜“ äº‘ éŸ³ä¹ å¬ acute Ï‰\n",
      "å‚» é€¼\n",
      "å¥½å¬ à¸‡ â€¢ Ì€ â€¢ Ì à¸‡\n",
      "å•ªå•ª å•ª\n",
      "è€å¸ˆ é‚£æ¬¡ å‘ æ–­è” ä¿¡æ¯ åŠä¸ª å¾®ä¿¡ æ‹‰ é»‘ è¿½ å¥³ç”Ÿ å¥³ç”Ÿ æ‹‰ é»‘ ä¸ä¸Š äº¤æµ\n",
      "å¹¿å‘Š ä¸“é¢˜ é…éŸ³ æœ‰å£° è¯»ç‰© å½•åˆ¶ æ¥ ç‰ˆæƒ åˆä½œé¡¹ç›® ä¸œç¯± QQ 407615201 å¾®ä¿¡ CJDLX9\n",
      "å…«å¤« å°è¯´ èµ·ç‚¹ hx ç™¾åº¦ äº‘\n",
      "QQ\n",
      "å¾®ä¿¡ é™Œç”Ÿäºº åŒå­¦ åŒäº‹ äº²äºº\n",
      "è›‹è§£ åˆ›ä¸š å…¬ä¼— å· å®å\n",
      "æ˜¨æ™š æ‰‹æœº è¡¥å ä¸€å—\n",
      "å–œç‚¹ å¬ä¹¦å–œ é’» é€ ä¸»æ’­ ç¤¼ç‰©\n",
      "ä¸ƒå¾‹ â€¢ äººæ°‘è§£æ”¾å†› å é¢† å—äº¬ æ¯›æ³½ä¸œ\n",
      "å‚» é€¼ å¹¿å‘Š\n",
      "emomenemodemomoemodooomomon\n",
      "IQ å‘ QQ\n",
      "ä¸é”™ à¹“ acute â•° â•¯ à¹“ â™¡\n",
      "å¿«ç‚¹ ä¸€åƒ äº”å å‘é£Ÿ ç© æ·˜å®ç½‘\n",
      "QQ ç®—äº†\n",
      "è‰æ³¥é©¬ å†™ çœŸä»–å¦ˆ laj\n",
      "ä¹ è¿‘å¹³ è°ˆ æ²»å›½ ç†æ”¿ ç¬¬äºŒå·\n",
      "ç»¿èŒ¶ å©Š æ¸£ ç”· æœ€é…\n",
      "å¦®å­ ç±³å°å…° ç»¿èŒ¶ å©Š æ€’ æ€’ æ€’ æ€’\n",
      "å• å®‡èˆª åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "ç”·ä¸»æœ‰ ç—… æ¸£ ç”· ç»¿èŒ¶ å©Š è¯¥æ­»\n",
      "è”æ\n",
      "é€†å¿ƒ ä¹±ä¼¦ æ±—\n",
      "ç»æµå­¦\n",
      "zhong é¸£ sq åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "ä¼é¹…\n",
      "qq å¾®ä¿¡\n",
      "æ³°è¯­ å­¦ä¹  è”ç›Ÿ à¸‚ à¸­ à¸š à¸„ à¸¸ à¸“ à¸¡ à¸² à¸ à¸• à¸£ à¸± à¸š\n",
      "å¾®ä¿¡\n",
      "QQ éŸ³ä¹ å¬\n",
      "å‚» é€¼ åºŸè¯è¿ç¯‡\n",
      "ä¸»è§’ å‚» é€¼ å¦ˆ æ‰‹æœº ä»æ¥ä¸ å¼€æœº\n",
      "nan\n",
      "å›½äºº æš´åŠ¨ éªŠå±± çƒ½ç«\n",
      "å¥½å¬ æ°‘å›½ å‰§ æµ·æŠ¥ ç”» å¥½å¸… å®‰ç§€å°˜ çœŸæ£’ é›†ç­– ç¼– å¯¼ åæœŸ ç¾å·¥ æ­Œæ›² å¡«è¯ ä¸€èº« è¿™éƒ¨ å‰§ é…éŸ³ é˜µå®¹ å¼º ä¸»å½¹ CV å£°å£°æ…¢ å—“éŸ³ æ¼”ç» è§’è‰² åˆ°ä½ æœŸå¾… ç¬¬äºŒæœŸ ç²¾å½© good good good\n",
      "æ’• é€¼ å¤§æˆ˜\n",
      "ç‹ æ¡ƒå­ MOMO é…± é—å¿˜ æ—¶å…‰ èŠ±å¿ƒ èŠ±å¿ƒ åäº”å² å°‘å¹´\n",
      "å®‹ç¥–è‹± å”±\n",
      "è°¢è°¢ å–œæ¬¢ à¹‘ â€¢ â€¢ à¹‘\n",
      "é‡è¯» æ¯›æ³½ä¸œ è¯—æ­Œ çƒ­è¡€ æ¾æ¹ƒ\n",
      "é«˜ä¸­ åˆšæœ‰ QQ ç½‘å§ é™Œç”Ÿäºº ä¸åœ å‘ ç”·æ€§ ç”Ÿæ®–å™¨å®˜ é‡ç‚¹ åŒçƒ¦ å–Š ä¸€åŒ åŒå­¦ ç¥ç»ç—… å‘ æ ‘æ ¹ å¹²å˜› åŒå­¦ å‘Šè¯‰ ä¸»æœº æŒ‰é’® çœŸ ä¸œè¥¿ å¾ˆå‚» ç¬¬ä¸€æ¬¡ ç•™è¨€\n",
      "å§æ§½ å‚» é€¼\n",
      "ä½ å¥½ å–œæ¬¢ ç¾ä¼¦æ–‡æ®µ èƒ½åŠ  å¾®ä¿¡ å¥½å‹ å¾® ä¿¡å· qlshipinlinlin æœŸå¾… åˆ°æ¥\n",
      "å¤ä¹  å•å…ƒ\n",
      "à°  àµ  à°  ä¸€è„¸ è¿·èŒ«\n",
      "éªšè´§\n",
      "ä¸»è§’ å‚» é€¼ ä¸¤ä¸ª ç¾å¥³ æ ä¸»è§’ å†™ ç‹—è¡€\n",
      "ä¼é¹… å®¡æ ¸ è´¹åŠ²\n",
      "å¥³ä¸» å‚» é€¼\n",
      "æˆ‘æ€ª ä¸å¥½æ„æ€ è„¸çº¢ à¸… à¸…\n",
      "æŠšé¡º ç»ˆäº ä¸‹é›ª å¥½é›ªå¥½ å¿ƒæƒ… ç¥ å…³æ³¨ æ™“æ¥  å¿ƒæƒ… åŠ æ²¹ à¸‡ â€¢ Ì€ â€¢ Ì à¸‡\n",
      "7x6drmehnwyuyhf9eucd åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "è€å¸ˆ å¾® ä¿¡å· 18647800234 éº»çƒ¦\n",
      "à½– à½¦ à½˜ à¼‹ à½‚ à¾² à½´ à½– à¼‹ à½– à½¦ à¾Ÿ à½“ à¼‹ à½  à½› à½² à½“ à¼\n",
      "æ°¸ä¸ åˆ«ç¦» äº‘é¡¶ Radio åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "à²¡ Ï‰ à²¡\n",
      "äº² åŠ å¾®ä¿¡ å…è´¹ è·å– è¯¾ç¨‹ ä¹¦å‘†å­\n",
      "æ¹¿èº« è¯±æƒ‘\n",
      "ğŸ™ˆ è‰²å³æ˜¯ç©º\n",
      "å¤§åŠå¤© å¬ å®Œ æ£’æ£’ å“’ é¢œ åŠ æ²¹ à¸‡ â€¢ Ì€ â€¢ Ì à¸‡\n",
      "éªšæµª è´±\n",
      "è¯·é—® é”™è¿‡ æ‰¾ ä¸åˆ° å¶ å§å§ è´Ÿè´£ æ‰¾ä¸ª ä¼¢ å«–å¨¼ å« å«\n",
      "å›å¤ åšå¼º å§‘å¨˜ ma è¿™æ˜¯ æœ€å°‘ å¬ 0.3 å–œç‚¹ ä¸€é›†\n",
      "é’å¹´ å£°éŸ³ ä¸€è¾ˆå­ è¯´å¥½ æ™®é€šè¯ é€€ä¼‘ åå¹´ å†æ¥å­¦ çº¸è´¨ æ–‡ç¨¿ å‘ ç”µå­ç‰ˆ 627638960 qq cOm è°¢è°¢ä½ ä»¬\n",
      "è½¬è½®æ‰‹æª\n",
      "å¤ª å¼€å¿ƒ å¥½å¥½ å¬ à¸‡ â€¢ Ì€ â€¢ Ì à¸‡\n",
      "æ‚¨å¥½ åŠ  å¾®ä¿¡ 15919401850\n",
      "å¾®ä¿¡\n",
      "å¼€ç©ç¬‘ äºŒå“¥ å«–å¨¼ ç»™é’±\n",
      "è”æ çƒ­\n",
      "æ¥ä¸¥ç» è®²è®° é‡Šé¢˜ ä½› å¦‚æ¥ åå· ..... 001\n",
      "mc å¤©ä½‘ å¹ç‰› å¹ç‰› å›½ç²¹ æ‰¯ å…³ç³» å›½ç²¹ ä½å»‰\n",
      "é¢é«˜ seo é¢é«˜ ç½‘ç«™ ä¼˜åŒ– å®‹ç¥–è‹±\n",
      "ç”ŸåŠ¨ è¾¾æ„ æ„Ÿè°¢ è€å¸ˆ æµ‡çŒ â— acute à²² â— å¤šå…ƒ å’Œè° äººç”Ÿ æœ€ç¾ åº•è‰²\n",
      "æŒº å¥½çœ‹ æ¯ ä¸‰è§‚ å¥³ä¸» å…¸å‹ ç©¿è¶Š ç»¿èŒ¶ å©Š çƒ‚ ä¹¦ æ±—\n",
      "ğŸ£ è–„è· å°å§ ğŸ£ æ¥åˆ° FM1387320 ğŸˆ ä¸€ç§ å–œæ¬¢ è®¢é˜… ğŸˆ ä¸€ç§ ä»»æ€§ åˆ· ç¤¼ç‰© ğŸˆ ä¸€ç§ å‘Šç™½ åˆ· ç¤¼ç‰© ğŸˆ æ„Ÿè°¢ ä¸€è·¯ ç›¸ä¼´ ğŸˆ qq ç¾¤ 213612483 â”Š ğŸ£ â”Š ğŸ£ â”Š ğŸ£ â”Š\n",
      "ğŸ id å¯†ç  å¿˜è®° å……å€¼ å¾®ä¿¡\n",
      "è”æ ç²‰ä¸\n",
      "å°å§å§ æˆ‘è¦ åŠ  QQ\n",
      "æ—¥å¼ å«åºŠ\n",
      "QQ æŒ‚æœº ä¸è®© ç™»\n",
      "ä¸‰æ™º ä¼ ä¹  å­° å¾®ä¿¡ è¯´ è‘£å¹³ æ•™æˆ 365 ç²¾è¯» ä¼ ä¹ å½• è®¡åˆ’ ç¬¬äºŒå±Š ä¸­å›½ é˜³æ˜å¿ƒ å­¦ é«˜å³°è®ºå› é—­å¹• é¦–å‘ è®ºå› å•¥æ—¶å€™\n",
      "å¥¥ç‰¹æ›¼ é™ˆ kygew åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "nan\n",
      "æ­Œè¯ æ”¹æˆ ä¸¢ è€æ¯ æ‰‘è¡—\n",
      "å›å¿†èµ· åå¹´ å‰ é‡åº† å‡ºå‘ è‡ªé©¾ å—åŒ— ç¼° å‡ºå¤´ å›æ¥ æ´— å‡ ç™¾å¼  ç…§ç‰‡ æ¯åˆ° åœ°æ–¹ ç½‘ç»œ ç¬”è®°æœ¬ç”µè„‘ qq ç©ºé—´ åŒæ­¥ å‘äº› å½“å¤© ç…§ç‰‡ åˆšåˆš å² å¥³å„¿ è‡ªé©¾ æ–°è¥¿å…° å—åŒ— å²› ä¸€ä¸ªæœˆ é©±è½¦ ä¸€å…± 4000 å¤šå…¬é‡Œ ä¸ç®— è·¨ åº“å…‹ æµ·å³¡ æ¸¡è½® æ™¯ç‚¹ è¦ç¥¨ å ç»Ÿä¸€ å¤§å·´ éœæ¯”ç‰¹ å±¯ ç§äºº é¢†åœ° çˆ± å»ä¸å» é‘« ç¼° æµ· å†…é™† å¥½ç¾ å–€çº³æ–¯ å·´éŸ³ å¤©é¹…æ¹– å‡ åª é¹… æ–°è¥¿å…° å¤©é¹… æˆç¾ é›¶è·ç¦» æ¥è§¦ æŠ“ç´§ æŠ± åˆå½± ä¸€ç‚¹ å¤¸å¤§ æ–°è¥¿å…° æ²³æµ æ¹–æ³Š ä¼—å¤š ä½ æ¹–è¾¹ æŒº ä¸»æ’­ è®°å½• å†™ç‚¹ ä¸œè¥¿ è°¢è°¢ å›å¿† è¾½é˜” é‘« ç¼°\n",
      "åªä¼š ç© QQ é£è½¦\n",
      "å¥³ä¸»è§’ å‚» é€¼ ç™½é•¿ å¼ å˜´\n",
      "ä¸è¨€ æ˜å¤© è¿™å”å¸¦ æ‰¾ å°å§\n",
      "Vickey ç§äº« æ¡¥ åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "JimmyAndJudy è®²æ•…äº‹ åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "Tobetheperfectperson åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "è°¢è°¢ æœ‰ç©º V ä¿¡ç¾¤ è½¬è½¬ æœ‰æ¶› å“¥ èº«å½±\n",
      "ä½œè€… å‚» é€¼ é€¼ å¢¨è¿¹ ç¼– æ•…äº‹ ç´¯ä¸ª çŠŠå­ å¥½è¯„ sb\n",
      "æˆ‘å‘ Ù© à¹‘ o à¹‘ Û¶ è¯´ æ•æ„Ÿ è¯æ±‡\n",
      "æ¸©å®¤ é‡Œ å°éœ¸ç‹ åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "æœ‰å¾® ä¿¡ç¾¤\n",
      "äº²å­ é˜…è¯» å®è·µè€… å€¡å¯¼è€… å„¿ç«¥ æ—©æœŸ é˜…è¯» æ¨å¹¿\n",
      "æ“ å¦ˆ å¬ å¬\n",
      "å›¾å›¾ ä¸€ç¬‘å€¾åŸ çˆ± çˆ± çˆ± æ„ŸåŠ¨ à²¥ à²¥\n",
      "æ³è¯· è€å¸ˆ å¸ƒæ–½ ä¸€ä»½ è®²ä¹‰ æ„Ÿè°¢ è€å¸ˆ qq1152267164\n",
      "æˆ‘ä¿© åªæ‰‹ æŠ¢\n",
      "å¹¼ç¨šå›­ ä½©å¥‡ 2s åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "æ–‡å„¿ ä½ å¥½ æ„å‘ åŠ  å…¬ä¼š æ·»åŠ  å¾®ä¿¡\n",
      "å¿«ç‚¹ ç‹— æ‰ å‚» é€¼ çŒªè„š\n",
      "3brtkrivd95kpw8teoke åšä¸» æ’­ ä¸“è¾‘ æ¨ å¹¿ äº‘ æ¨ ç½‘ç»œ æä¾› ä½ä»· é«˜æ•ˆ å¿«é€Ÿ æé«˜ ä¸»æ’­ ç»¼åˆæ’å ä¸šåŠ¡ è¯¦è¯¢ å¾® ä¿¡ ZHUBOUP\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "# pre = tf.argmax(train_label,1)\n",
    "# print(pre)\n",
    "true_label = train[\"flag\"]\n",
    "train_data = train[\"sentence\"]\n",
    "# print(true_label)\n",
    "# print(train_data[6])\n",
    "# print(true_label[6])\n",
    "n = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    if(pre[i] != true_label[i]):\n",
    "        n = n + 1\n",
    "    if(pre[i] != true_label[i] and pre[i] == 1):\n",
    "        print(train_data[i])\n",
    "        \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
