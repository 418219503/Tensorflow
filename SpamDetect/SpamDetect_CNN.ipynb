{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "model = gensim.models.Word2Vec.load('spamDetect_20180217.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二天 0.962252557278\n",
      "刚买 0.961527764797\n",
      "够不着 0.961062729359\n",
      "没事 0.956673383713\n",
      "xinjiang 0.955379366875\n",
      "叶 0.955097436905\n",
      "追 0.954735636711\n",
      "去程 0.954279601574\n",
      "怀里 0.95322817564\n",
      "留在 0.95120036602\n",
      "<type 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(u'花')\n",
    "for word in result:\n",
    "    print word[0],word[1]\n",
    "print(type(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8019 lines...\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "7989    1\n",
      "7990    1\n",
      "7991    1\n",
      "7992    1\n",
      "7993    1\n",
      "7994    1\n",
      "7995    1\n",
      "7996    1\n",
      "7997    1\n",
      "7998    1\n",
      "7999    1\n",
      "8000    1\n",
      "8001    1\n",
      "8002    1\n",
      "8003    1\n",
      "8004    1\n",
      "8005    1\n",
      "8006    1\n",
      "8007    1\n",
      "8008    1\n",
      "8009    1\n",
      "8010    1\n",
      "8011    1\n",
      "8012    1\n",
      "8013    1\n",
      "8014    1\n",
      "8015    1\n",
      "8016    1\n",
      "8017    1\n",
      "8018    1\n",
      "Name: flag, Length: 8019, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# train = pd.read_csv( \"shuf_spam_train.csv\", header=0, delimiter=\",\", quoting=2)\n",
    "train = pd.read_csv( \"spam_train_5_5.csv\")\n",
    "print(\"Read %d lines...\" % (train[\"sentence\"].shape))\n",
    "\n",
    "train_data = train[\"sentence\"]\n",
    "train_label = train[\"flag\"]\n",
    "\n",
    "# print(train_data[0])\n",
    "print(train_label)\n",
    "# print(train_label)\n",
    "# print(train_data)\n",
    "# for i in range(len(train)):\n",
    "#     if(train_label[i] == 0.):\n",
    "#         print(train_data[i])\n",
    "        \n",
    "# print(train_data[20077])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jieba\n",
    "# import io\n",
    "\n",
    "# stopwords = [line.strip() for line in io.open('stopwords.txt', 'r', encoding='utf-8').readlines()] \n",
    "# stopwords = set(stopwords)\n",
    "\n",
    "# for i in range(len(neg_sentences)):\n",
    "#     line_seg = jieba.cut(neg_sentences[i].strip())\n",
    "#     outstr = \"\"\n",
    "#     for word in line_seg:  \n",
    "#         if word not in stopwords:\n",
    "#             outstr += word  \n",
    "#             outstr += \" \"\n",
    "        \n",
    "#     neg_sentences[i] = outstr.strip()\n",
    "#     outstr = \"\"\n",
    "#     if(i % 10000 == 0):\n",
    "#         print(\"neg deal %s of %s\" % (i,len(neg_sentences)))\n",
    "        \n",
    "# for i in range(len(pos_sentences)):\n",
    "#     line_seg = jieba.cut(pos_sentences[i].strip())\n",
    "#     outstr = \"\"\n",
    "#     for word in line_seg:  \n",
    "#         if word not in stopwords:\n",
    "#             outstr += word  \n",
    "#             outstr += \" \"\n",
    "        \n",
    "#     pos_sentences[i] = outstr.strip()\n",
    "#     outstr = \"\"\n",
    "#     if(i % 10000 == 0):\n",
    "#         print(\"pos deal %s of %s\" % (i,len(pos_sentences)))\n",
    "\n",
    "# del stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 324\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "\n",
    "    nwords = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for word in words:\n",
    "        if word.decode(\"utf-8\") in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word.decode(\"utf-8\")])\n",
    "    if(nwords > 0):\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(sentences, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    print(len(sentences))\n",
    "    reviewFeatureVecs = np.zeros((len(sentences),num_features),dtype=\"float32\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "       if(isinstance(sentence,basestring)==False):\n",
    "            sentence=\" \"\n",
    "       if counter%1000 == 0:\n",
    "           print \"Review %d of %d\" % (counter, len(sentences))\n",
    "       words = sentence.split(\" \")\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(words, model, num_features)\n",
    "\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "回复 韵 lt 版权 网易 云 音乐 找\n",
      "8019\n",
      "Review 0 of 8019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 8019\n",
      "Review 2000 of 8019\n",
      "Review 3000 of 8019\n",
      "Review 4000 of 8019\n",
      "Review 5000 of 8019\n",
      "Review 6000 of 8019\n",
      "Review 7000 of 8019\n",
      "Review 8000 of 8019\n",
      "<type 'numpy.ndarray'>\n",
      "8019\n",
      "[[-0.05196826  0.62578809  0.33745331 ...,  0.07365704 -0.18128303\n",
      "   0.05877255]\n",
      " [-0.13448544 -0.15526298  0.27640295 ..., -0.28622359 -0.1406382\n",
      "  -0.25426164]\n",
      " [-0.04657002  0.63925362 -0.04474411 ..., -0.41634336 -0.48093891\n",
      "   0.03007146]\n",
      " ..., \n",
      " [ 0.04947954  0.23000774  0.35520238 ...,  0.10404185 -0.23208822\n",
      "   0.19359264]\n",
      " [ 0.01632851  0.13351597  0.69430727 ...,  0.09240357 -0.31910026\n",
      "   0.12541388]\n",
      " [-0.04290846  0.18934393  0.69835955 ...,  0.14514384 -0.36947876\n",
      "   0.15277256]]\n"
     ]
    }
   ],
   "source": [
    "train_clean_data = []\n",
    "for review in train[\"sentence\"]:\n",
    "    train_clean_data.append(review)\n",
    "print(type(train_clean_data)) \n",
    "print(train_clean_data[0])\n",
    "train_clean_data = getAvgFeatureVecs(train_clean_data, model, num_features )\n",
    "print(type(train_clean_data))\n",
    "print(len(train_clean_data))\n",
    "print(train_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_weight_variable(layer_name, shape):\n",
    "    \"\"\" Retrieve an existing variable with the given layer name \n",
    "    \"\"\"\n",
    "    return tf.get_variable(layer_name, shape=shape, initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "def fc_weight_variable(layer_name, shape):\n",
    "    \"\"\" Retrieve an existing variable with the given layer name\n",
    "    \"\"\"\n",
    "    return tf.get_variable(layer_name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\" Creates a new bias variable\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.constant(0.0, shape=shape))\n",
    "\n",
    "#卷积层\n",
    "def conv_layer(input,               # 前一层的输出\n",
    "                layer_name,         # 当前层的名字\n",
    "                num_input_channels, # 前一层的神经元个数\n",
    "                filter_size,        # 滤波器长度／宽度\n",
    "                num_filters,        # 滤波器个数\n",
    "                pooling=True):      # 是否使用2X2的池化层\n",
    "\n",
    "    # 卷积层的权重属性\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # 创建当前层的权重\n",
    "    weights = conv_weight_variable(layer_name, shape=shape)\n",
    "    \n",
    "    # 构造偏移量\n",
    "    biases = bias_variable(shape=[num_filters])\n",
    "\n",
    "    # 创建一个滤波器\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME') # with zero padding\n",
    "\n",
    "    layer += biases\n",
    "    \n",
    "    # 使用RELU激活函数\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # 是否使用池化层，这里使用maxpool\n",
    "    if pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "#降低数据维度到一维\n",
    "def flatten_layer(layer):\n",
    "    # 获取层结构属性\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # 计算层的features数量: img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # 重新将层构造成 [图像个数, features数量].\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer_flat, num_features\n",
    "\n",
    "\n",
    "#全链接层\n",
    "def fc_layer(input,        # 前一层输出\n",
    "             layer_name,   # 当前层的名字\n",
    "             num_inputs,   # 前一层的输入个数\n",
    "             num_outputs,  # 输出个数\n",
    "             relu=True):   # 是否使用RELU激活函数\n",
    "\n",
    "    # 构造滤波器和偏移量\n",
    "    weights = fc_weight_variable(layer_name, shape=[num_inputs, num_outputs])\n",
    "    biases = bias_variable(shape=[num_outputs])\n",
    "\n",
    "    # 计算\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # 是否使用RELU激活函数\n",
    "    if relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "    return layer\n",
    "\n",
    "# 第一层卷积层\n",
    "filter_size1 = 4         # 滤波器5*5\n",
    "num_filters1 = 32         # 隐藏神经元个数\n",
    "\n",
    "# 第二层卷积层\n",
    "filter_size2 = 4          # 滤波器5*5\n",
    "num_filters2 = 64         # 隐藏神经元个数\n",
    "\n",
    "# 全链接层神经元个数\n",
    "fc_size = 128\n",
    "\n",
    "num_channels = 1\n",
    "width_v = 18\n",
    "long_v = 18\n",
    "num_classes = 2\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, width_v, long_v, num_channels), name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "#y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "y_true_cls = tf.argmax(y_true, 1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#计算第一个卷积层\n",
    "conv_1, w_c1 = conv_layer(input=x,\n",
    "                          layer_name=\"conv_1\",\n",
    "                          num_input_channels=num_channels,\n",
    "                          filter_size=filter_size1,\n",
    "                          num_filters=num_filters1, pooling=True)\n",
    "\n",
    "conv_1\n",
    "\n",
    "#计算第二个卷积层\n",
    "conv_2, w_c2 = conv_layer(input=conv_1,\n",
    "                          layer_name=\"conv_2\",\n",
    "                          num_input_channels=num_filters1,\n",
    "                          filter_size=filter_size2,\n",
    "                          num_filters=num_filters2,\n",
    "                          pooling=True)\n",
    "\n",
    "# 使用dropout层，避免过拟合\n",
    "dropout = tf.nn.dropout(conv_2, keep_prob)\n",
    "\n",
    "dropout\n",
    "\n",
    "#降低到一维数据\n",
    "layer_flat, num_features = flatten_layer(dropout)\n",
    "\n",
    "layer_flat\n",
    "\n",
    "#全链接层\n",
    "fc_1 = fc_layer(input=layer_flat,\n",
    "                layer_name=\"fc_1\",\n",
    "                num_inputs=num_features,\n",
    "                num_outputs=fc_size,\n",
    "                relu=True)\n",
    "\n",
    "fc_1\n",
    "\n",
    "fc_2 = fc_layer(input=fc_1,\n",
    "                layer_name=\"fc_2\",\n",
    "                num_inputs=fc_size,\n",
    "                num_outputs=num_classes,\n",
    "                relu=False)\n",
    "\n",
    "fc_2\n",
    "\n",
    "#使用softmax\n",
    "y_pred = tf.nn.softmax(fc_2)\n",
    "\n",
    "# The class-number is the index of the largest element.\n",
    "#y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "y_pred_cls = tf.argmax(y_pred, 1)\n",
    "\n",
    "# Calcualte the cross-entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc_2, labels=y_true)\n",
    "\n",
    "# Take the average of the cross-entropy for all the image classifications.\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Global step is required to compute the decayed learning rate\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "# Apply exponential decay to the learning rate\n",
    "learning_rate = tf.train.exponential_decay(0.001, global_step, 10000, 0.96, staircase=True)\n",
    "\n",
    "# 使用AdagradOptimizer自适应学习率和BP更新权重\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "\n",
    "# 计算误差\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "\n",
    "# 用方差表示误差\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# 每一批训练样本的个数 \n",
    "batch_size = 128\n",
    "\n",
    "# dropout比例\n",
    "dropout = 0.5\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "label = train[\"flag\"]\n",
    "\n",
    "#训练函数\n",
    "def optimize(num_iterations, display_step):\n",
    "    #处理标签数据\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    "    \n",
    "#     for i in range(len(label)):\n",
    "#         label[i] = int(label[i])\n",
    "#         if i%1000 == 0:\n",
    "#            print \"deal %d of %d\" % (i, len(label))\n",
    "        \n",
    "    train_data = train_clean_data[0 : int(len(train_clean_data) * 1),:]\n",
    "    train_label = label[0 : int(len(train_clean_data) * 1)]\n",
    "    \n",
    "#     for l in train_label:\n",
    "#         print(l)\n",
    "    \n",
    "    # 使用ont-hot编码\n",
    "    enc = OneHotEncoder().fit(train_label.reshape(-1, 1))\n",
    "    train_label = enc.transform(train_label.reshape(-1, 1)).toarray()\n",
    "    \n",
    "#     print(train_label)\n",
    "    \n",
    "    global total_iterations\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(0,num_iterations):\n",
    "\n",
    "        offset = (step * batch_size) % (int(len(train_clean_data) * 1) - batch_size)\n",
    "        \n",
    "        batch_x = train_clean_data[offset:(offset + batch_size),:]\n",
    "        batch_y = train_label[offset:(offset + batch_size),:]\n",
    "        \n",
    "        batch_x = batch_x.reshape(-1,width_v,long_v,num_channels)\n",
    "        \n",
    "        feed_dict_train = {x: batch_x, y_true: batch_y, keep_prob: dropout}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "#         print(\"12312312313,\",batch_x)\n",
    "        if step % display_step == 0:\n",
    "#             pre = session.run(y_pred_cls, {x:batch_x , y_true:batch_y , keep_prob: 1.0})\n",
    "#             softmax = session.run(y_pred, {x:batch_x , y_true:batch_y , keep_prob: 1.0})\n",
    "#             print(\"softmax:\",softmax)\n",
    "#             print(\"pre:\",pre)\n",
    "#             print(\"true:\",batch_y)\n",
    "            \n",
    "            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            print(\"Minibatch accuracy at step %d: %.4f\" % (step, batch_acc))\n",
    "            \n",
    "            \n",
    "#             validation_acc = session.run(accuracy, {x:test_data , y_true:test_label , keep_prob: 1.0})\n",
    "#             print(\"Validation accuracy: %.4f\" % validation_acc)\n",
    "\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    time_diff = time.time() - start_time\n",
    "    \n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_diff)))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore the model...\n",
      "INFO:tensorflow:Restoring parameters from /notebooks/SpamTrain/model_cnn_256_06/comment_cnn_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "if(os.path.exists(\"/notebooks/SpamTrain/model_cnn_256_06/checkpoint\") == False):\n",
    "    # increase num_iterations to get better accuracy = 5000 to 50000 &  display_step=500\n",
    "    optimize(num_iterations=(len(train_clean_data)-100), display_step=100)\n",
    "    saver_path = saver.save(session, \"/notebooks/SpamTrain/model_cnn_256_06/comment_cnn_model.ckpt\")  # 保存模型\n",
    "else:\n",
    "   print(\"restore the model...\")\n",
    "   saver.restore(session,'/notebooks/SpamTrain/model_cnn_256_06/comment_cnn_model.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8790\n",
      "Validation accuracy: 0.8790\n",
      "Validation accuracy: 0.8790\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    batch_x = train_clean_data[i * 1000: (i+1) * 1000,:]\n",
    "    batch_x = batch_x.reshape(-1,width_v,long_v,num_channels)\n",
    "    train_label = train[\"flag\"]\n",
    "    # print(train_label)\n",
    "    enc = OneHotEncoder().fit(train_label.reshape(-1, 1))\n",
    "    train_label = enc.transform(train_label.reshape(-1, 1)).toarray()\n",
    "    train_label = train_label[i * 1000: (i+1) * 1000]\n",
    "    # print(train_label)\n",
    "\n",
    "    validation_acc = session.run(accuracy, {x:batch_x , y_true:train_label , keep_prob: 1.0})\n",
    "    pre = session.run(y_pred_cls, {x:batch_x, keep_prob: 1.0})\n",
    "    print(\"Validation accuracy: %.4f\" % validation_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "绿茶 婊\n",
      "主播 喜欢 狗 管理 翻不起 浪来\n",
      "吊毛 道士 结结巴巴 傻 逼 样子\n",
      "安眠药\n",
      "加个 微信 美女\n",
      "女娲 苍老 师 旁边 跳舞 帮 我助 助威 mc 天佑 吹牛 吹牛 国粹 扯 关系 中国 国粹 低廉\n",
      "宋庆龄 信守诺言\n",
      "加 微信 私房照 nbsp nbsp 挖 鼻屎\n",
      "真 人称 不用 微信 苍白 谎言 无力 zhuzhang 权力 怀念 QQ 感觉 QQ 流氓\n",
      "你好 冒昧 打扰 做 正品 左 哼哼 Vans 阴险 万斯 耐克鞋 专柜 代 购阿迪 愤怒 NB 匡威 正品 体育运动 鞋服 价格 专 懒得 理 柜 实体店 说 白菜 小店 做 良心 正品 支持 形式 鉴定 交易 晕 终身 朋友 有意 V 💘 Vans8765 朋友圈 🈶 更新 打扰 请 海涵 🙏\n",
      "温情脉脉 momo 四声 做鬼脸\n",
      "Assen 捷唱过 风格 曲子 移步 网易 云 音乐 听 acute ω\n",
      "傻 逼\n",
      "好听 ง • ̀ • ́ ง\n",
      "啪啪 啪\n",
      "老师 那次 发 断联 信息 半个 微信 拉 黑 追 女生 女生 拉 黑 不上 交流\n",
      "广告 专题 配音 有声 读物 录制 接 版权 合作项目 东篱 QQ 407615201 微信 CJDLX9\n",
      "八夫 小说 起点 hx 百度 云\n",
      "QQ\n",
      "微信 陌生人 同学 同事 亲人\n",
      "蛋解 创业 公众 号 实名\n",
      "昨晚 手机 补坏 一块\n",
      "喜点 听书喜 钻 送 主播 礼物\n",
      "七律 • 人民解放军 占领 南京 毛泽东\n",
      "傻 逼 广告\n",
      "emomenemodemomoemodooomomon\n",
      "IQ 发 QQ\n",
      "不错 ๓ acute ╰ ╯ ๓ ♡\n",
      "快点 一千 五十 发食 玩 淘宝网\n",
      "QQ 算了\n",
      "草泥马 写 真他妈 laj\n",
      "习近平 谈 治国 理政 第二卷\n",
      "绿茶 婊 渣 男 最配\n",
      "妮子 米小兰 绿茶 婊 怒 怒 怒 怒\n",
      "吕 宇航 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "男主有 病 渣 男 绿茶 婊 该死\n",
      "荔枝\n",
      "逆心 乱伦 汗\n",
      "经济学\n",
      "zhong 鸣 sq 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "企鹅\n",
      "qq 微信\n",
      "泰语 学习 联盟 ข อ บ ค ุ ณ ม า ก ต ร ั บ\n",
      "微信\n",
      "QQ 音乐 听\n",
      "傻 逼 废话连篇\n",
      "主角 傻 逼 妈 手机 从来不 开机\n",
      "nan\n",
      "国人 暴动 骊山 烽火\n",
      "好听 民国 剧 海报 画 好帅 安秀尘 真棒 集策 编 导 后期 美工 歌曲 填词 一身 这部 剧 配音 阵容 强 主役 CV 声声慢 嗓音 演绎 角色 到位 期待 第二期 精彩 good good good\n",
      "撕 逼 大战\n",
      "王 桃子 MOMO 酱 遗忘 时光 花心 花心 十五岁 少年\n",
      "宋祖英 唱\n",
      "谢谢 喜欢 ๑ • • ๑\n",
      "重读 毛泽东 诗歌 热血 澎湃\n",
      "高中 刚有 QQ 网吧 陌生人 不停 发 男性 生殖器官 重点 厌烦 喊 一同 同学 神经病 发 树根 干嘛 同学 告诉 主机 按钮 真 东西 很傻 第一次 留言\n",
      "卧槽 傻 逼\n",
      "你好 喜欢 美伦文段 能加 微信 好友 微 信号 qlshipinlinlin 期待 到来\n",
      "复习 单元\n",
      "ఠ ൠ ఠ 一脸 迷茫\n",
      "骚货\n",
      "主角 傻 逼 两个 美女 搞 主角 写 狗血\n",
      "企鹅 审核 费劲\n",
      "女主 傻 逼\n",
      "我怪 不好意思 脸红 ฅ ฅ\n",
      "抚顺 终于 下雪 好雪好 心情 祝 关注 晓楠 心情 加油 ง • ̀ • ́ ง\n",
      "7x6drmehnwyuyhf9eucd 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "老师 微 信号 18647800234 麻烦\n",
      "བ ས མ ་ ག ྲ ུ བ ་ བ ས ྟ ན ་ འ ཛ ི ན །\n",
      "永不 别离 云顶 Radio 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "ಡ ω ಡ\n",
      "亲 加微信 免费 获取 课程 书呆子\n",
      "湿身 诱惑\n",
      "🙈 色即是空\n",
      "大半天 听 完 棒棒 哒 颜 加油 ง • ̀ • ́ ง\n",
      "骚浪 贱\n",
      "请问 错过 找 不到 叶 姐姐 负责 找个 伢 嫖娼 嫁 嫁\n",
      "回复 坚强 姑娘 ma 这是 最少 听 0.3 喜点 一集\n",
      "青年 声音 一辈子 说好 普通话 退休 十年 再来学 纸质 文稿 发 电子版 627638960 qq cOm 谢谢你们\n",
      "转轮手枪\n",
      "太 开心 好好 听 ง • ̀ • ́ ง\n",
      "您好 加 微信 15919401850\n",
      "微信\n",
      "开玩笑 二哥 嫖娼 给钱\n",
      "荔枝 热\n",
      "楞严经 讲记 释题 佛 如来 十号 ..... 001\n",
      "mc 天佑 吹牛 吹牛 国粹 扯 关系 国粹 低廉\n",
      "颐高 seo 颐高 网站 优化 宋祖英\n",
      "生动 达意 感谢 老师 浇灌 ◍ acute ಲ ◍ 多元 和谐 人生 最美 底色\n",
      "挺 好看 毁 三观 女主 典型 穿越 绿茶 婊 烂 书 汗\n",
      "🐣 薄荷 小姐 🐣 来到 FM1387320 🎈 一种 喜欢 订阅 🎈 一种 任性 刷 礼物 🎈 一种 告白 刷 礼物 🎈 感谢 一路 相伴 🎈 qq 群 213612483 ┊ 🐣 ┊ 🐣 ┊ 🐣 ┊\n",
      "🍎 id 密码 忘记 充值 微信\n",
      "荔枝 粉丝\n",
      "小姐姐 我要 加 QQ\n",
      "日式 叫床\n",
      "QQ 挂机 不让 登\n",
      "三智 传习 孰 微信 说 董平 教授 365 精读 传习录 计划 第二届 中国 阳明心 学 高峰论坛 闭幕 首发 论坛 啥时候\n",
      "奥特曼 陈 kygew 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "nan\n",
      "歌词 改成 丢 老母 扑街\n",
      "回忆起 十年 前 重庆 出发 自驾 南北 缰 出头 回来 洗 几百张 照片 每到 地方 网络 笔记本电脑 qq 空间 同步 发些 当天 照片 刚刚 岁 女儿 自驾 新西兰 南北 岛 一个月 驱车 一共 4000 多公里 不算 跨 库克 海峡 渡轮 景点 要票 坐 统一 大巴 霍比特 屯 私人 领地 爱 去不去 鑫 缰 海 内陆 好美 喀纳斯 巴音 天鹅湖 几只 鹅 新西兰 天鹅 成灾 零距离 接触 抓紧 抱 合影 一点 夸大 新西兰 河流 湖泊 众多 住 湖边 挺 主播 记录 写点 东西 谢谢 回忆 辽阔 鑫 缰\n",
      "只会 玩 QQ 飞车\n",
      "女主角 傻 逼 白长 张嘴\n",
      "不言 明天 这叔带 找 小姐\n",
      "Vickey 私享 桥 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "JimmyAndJudy 讲故事 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "Tobetheperfectperson 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "谢谢 有空 V 信群 转转 有涛 哥 身影\n",
      "作者 傻 逼 逼 墨迹 编 故事 累个 犊子 好评 sb\n",
      "我发 ٩ ๑ o ๑ ۶ 说 敏感 词汇\n",
      "温室 里 小霸王 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "有微 信群\n",
      "亲子 阅读 实践者 倡导者 儿童 早期 阅读 推广\n",
      "操 妈 听 听\n",
      "图图 一笑倾城 爱 爱 爱 感动 ಥ ಥ\n",
      "恳请 老师 布施 一份 讲义 感谢 老师 qq1152267164\n",
      "我俩 只手 抢\n",
      "幼稚园 佩奇 2s 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "文儿 你好 意向 加 公会 添加 微信\n",
      "快点 狗 掉 傻 逼 猪脚\n",
      "3brtkrivd95kpw8teoke 做主 播 专辑 推 广 云 推 网络 提供 低价 高效 快速 提高 主播 综合排名 业务 详询 微 信 ZHUBOUP\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "# pre = tf.argmax(train_label,1)\n",
    "# print(pre)\n",
    "true_label = train[\"flag\"]\n",
    "train_data = train[\"sentence\"]\n",
    "# print(true_label)\n",
    "# print(train_data[6])\n",
    "# print(true_label[6])\n",
    "n = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    if(pre[i] != true_label[i]):\n",
    "        n = n + 1\n",
    "    if(pre[i] != true_label[i] and pre[i] == 1):\n",
    "        print(train_data[i])\n",
    "        \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
