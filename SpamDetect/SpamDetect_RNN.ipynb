{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "model = gensim.models.Word2Vec.load('spamDetect_20180217.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8019 lines...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# train = pd.read_csv( \"shuf_spam_train.csv\", header=0, delimiter=\",\", quoting=2)\n",
    "train = pd.read_csv( \"spam_train_5_5.csv\")\n",
    "print(\"Read %d lines...\" % (train[\"sentence\"].shape))\n",
    "\n",
    "train_data = train[\"sentence\"]\n",
    "train_label = train[\"flag\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 324\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "\n",
    "    nwords = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for word in words:\n",
    "        if word.decode(\"utf-8\") in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word.decode(\"utf-8\")])\n",
    "#             print(\"========\",word,\":\",model[word.decode(\"utf-8\")])\n",
    "    if(nwords > 0):\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "        \n",
    "#     print(featureVec)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(sentences, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    print(len(sentences))\n",
    "    reviewFeatureVecs = np.zeros((len(sentences),num_features),dtype=\"float32\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "       if(isinstance(sentence,basestring)==False):\n",
    "            sentence=\" \"\n",
    "       if counter%1000 == 0:\n",
    "           print \"Review %d of %d\" % (counter, len(sentences))\n",
    "       words = sentence.split(\" \")\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(words, model, num_features)\n",
    "\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8019\n",
      "Review 0 of 8019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 8019\n",
      "Review 2000 of 8019\n",
      "Review 3000 of 8019\n",
      "Review 4000 of 8019\n",
      "Review 5000 of 8019\n",
      "Review 6000 of 8019\n",
      "Review 7000 of 8019\n",
      "Review 8000 of 8019\n"
     ]
    }
   ],
   "source": [
    "train_clean_data = []\n",
    "for review in train[\"sentence\"]:\n",
    "    train_clean_data.append(review)\n",
    "# print(type(train_clean_data)) \n",
    "# print(train_clean_data[0])\n",
    "train_clean_data = getAvgFeatureVecs(train_clean_data, model, num_features)\n",
    "# print(type(train_clean_data))\n",
    "# print(len(train_clean_data))\n",
    "# print(train_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13448544 -0.15526298  0.27640295  0.24393681 -0.031809    0.47941011\n",
      " -0.22217706  0.06064174 -0.20854169 -0.25044388 -0.07815401 -0.36584005\n",
      "  0.13301717 -0.33904946  0.06741552  0.153918   -0.07450511 -0.00255099\n",
      " -0.0229926   0.01189698  0.13944772 -0.16746029 -0.12637469 -0.34101805\n",
      " -0.16907683 -0.22200537 -0.00183525  0.11370945 -0.06828809 -0.03814153\n",
      " -0.02239798  0.16260451 -0.13899696  0.02925395 -0.07704003  0.00144653\n",
      " -0.04167575  0.12710071  0.06970742 -0.04279945 -0.1034698  -0.01272895\n",
      " -0.07456801  0.03568219 -0.1139776   0.4366076   0.13906632 -0.07055129\n",
      " -0.06471727  0.14301565 -0.20270628 -0.01353151 -0.28606248  0.11344427\n",
      " -0.05270681 -0.07796086  0.11920242 -0.2101891   0.07080494 -0.03920368\n",
      " -0.14051589 -0.12718305  0.24923721  0.2017079   0.19455475 -0.22270826\n",
      "  0.00910456  0.01823404  0.0525796   0.21754676 -0.16793996  0.08711456\n",
      "  0.07909136 -0.14023148 -0.02585023  0.29666746  0.16141331 -0.19616255\n",
      "  0.01133293 -0.01034601  0.23769563  0.26301605 -0.12038835 -0.10057741\n",
      "  0.19508247 -0.1421898  -0.01319015 -0.09271426  0.14374334 -0.13856086\n",
      " -0.21417119 -0.06861473 -0.30899021 -0.01583019 -0.11911713 -0.2070758\n",
      " -0.19881199  0.20158675 -0.0215104  -0.200784   -0.15272906 -0.0712617\n",
      " -0.00489839  0.00619499 -0.15628797  0.17658734  0.24058804 -0.33791399\n",
      "  0.32823694 -0.26570266  0.15346447 -0.04073823  0.18012944  0.00213488\n",
      " -0.07150075  0.06778381 -0.39864904 -0.00440429  0.18275881 -0.01352375\n",
      " -0.10473266 -0.18520632 -0.2431201  -0.18085618  0.16707864  0.22580576\n",
      "  0.04919728 -0.25554302  0.32865363 -0.36235955 -0.08409854 -0.06959924\n",
      " -0.02158564  0.1724662  -0.16058543 -0.09579413 -0.0088158   0.0687304\n",
      "  0.07104912  0.16828552 -0.06490887  0.23003969  0.00858108  0.0761345\n",
      "  0.0833237   0.04664256 -0.0811648   0.32718259  0.03176504 -0.28113419\n",
      " -0.02895404  0.09196193  0.11611918  0.1372034  -0.24998575  0.31597516\n",
      " -0.22996408  0.07153191 -0.01234623  0.07906514 -0.03250011 -0.1200698\n",
      " -0.16498244  0.31410944  0.05156414 -0.00655377 -0.06754364 -0.22507796\n",
      " -0.05500419  0.26982161 -0.1702331  -0.11134513 -0.08310188 -0.10245387\n",
      " -0.15751988 -0.17986365  0.09207748 -0.15770036  0.18470207  0.13324898\n",
      "  0.0705004  -0.19485658 -0.07531852  0.00635524  0.27748793 -0.12605736\n",
      " -0.10581504  0.2115446   0.10153814  0.13129565  0.13189782 -0.1461608\n",
      "  0.27115008  0.44785836 -0.02412939  0.26764804  0.17552245 -0.12904412\n",
      "  0.01212176 -0.06947646  0.16336486  0.03478027 -0.09411415 -0.11189057\n",
      "  0.03634082  0.04265679 -0.09250003  0.0937765   0.18651737  0.27348384\n",
      " -0.10998372  0.17498246  0.18932739 -0.01352445 -0.1639598   0.01969145\n",
      "  0.09994213 -0.51601338  0.31610644 -0.08713923  0.233257    0.08237537\n",
      " -0.00861661  0.18664995  0.12895364 -0.10821804  0.18046725  0.07633242\n",
      " -0.13397621  0.0239581  -0.28468418  0.19129446  0.44494051  0.08691217\n",
      "  0.10194519 -0.00655821 -0.42788434 -0.23604138 -0.09339587 -0.07482907\n",
      " -0.03442968  0.19513007  0.181604    0.18505356 -0.21602643  0.09479868\n",
      " -0.11720222 -0.36051351 -0.18063304 -0.03318525 -0.12858687 -0.07853388\n",
      " -0.08857587  0.12023009 -0.01219314 -0.05302381  0.19221483  0.34700221\n",
      "  0.11778876  0.32780647  0.21740034 -0.28334343  0.0042651   0.44359818\n",
      "  0.09135266 -0.03766453 -0.50360465 -0.22238019 -0.17019266  0.0462488\n",
      "  0.45798883  0.10373513  0.05853742  0.37486002  0.06822003  0.12080956\n",
      " -0.05160632 -0.31739628 -0.13102192  0.01988909 -0.19315502  0.12826353\n",
      "  0.17469779  0.14090526 -0.16400294 -0.27728295 -0.13005614  0.01234534\n",
      " -0.35741916  0.06829016  0.03263417  0.13296291  0.2089534  -0.27154452\n",
      "  0.24348442  0.14043948 -0.1323441  -0.30637574 -0.06033672 -0.1737799\n",
      "  0.06386288  0.2177082   0.04042886  0.01990662  0.34923565  0.11958057\n",
      " -0.12773956  0.1264921  -0.21288361  0.02334154 -0.07582372 -0.10330852\n",
      " -0.00966077 -0.1201153  -0.00975936  0.05829326 -0.42446926  0.30409425\n",
      " -0.19569597 -0.05579742 -0.05360914 -0.28622359 -0.1406382  -0.25426164]\n",
      "绿茶 婊\n",
      "[ -1.22874044e-01  -1.56023517e-01   3.16639155e-01   2.81163126e-01\n",
      "  -4.72658649e-02   5.25827169e-01  -2.24088222e-01   6.25452176e-02\n",
      "  -2.26118594e-01  -2.67426103e-01  -8.59484300e-02  -3.69492292e-01\n",
      "   1.15674138e-01  -3.69340122e-01   9.33132172e-02   1.53916270e-01\n",
      "  -7.44015425e-02  -6.40301267e-03  -3.34962569e-02   2.05407534e-02\n",
      "   1.35447845e-01  -1.84745789e-01  -1.35484293e-01  -3.45059961e-01\n",
      "  -1.83763430e-01  -2.52867579e-01   1.53329317e-02   1.22784764e-01\n",
      "  -7.01924413e-02  -4.04029898e-02  -9.04041994e-03   1.92435041e-01\n",
      "  -1.43162429e-01   1.32622407e-03  -7.53799677e-02  -3.43777202e-02\n",
      "  -3.19071077e-02   1.25566959e-01   4.21670936e-02  -9.04039945e-03\n",
      "  -1.43158734e-01   3.84667609e-03  -7.83161893e-02   2.86007356e-02\n",
      "  -1.25641212e-01   4.93785799e-01   1.74966753e-01  -7.84737542e-02\n",
      "  -7.51065388e-02   1.74344897e-01  -1.99082360e-01   8.88425711e-05\n",
      "  -3.33597004e-01   9.50204954e-02  -7.19949007e-02  -8.30957443e-02\n",
      "   9.04799178e-02  -2.37248853e-01   7.36012235e-02  -4.70135547e-02\n",
      "  -1.58635110e-01  -1.47323206e-01   2.78282940e-01   2.01482490e-01\n",
      "   2.15408623e-01  -2.53871441e-01  -1.59483554e-03   1.00415647e-02\n",
      "   6.97098821e-02   2.39444017e-01  -1.93578929e-01   9.41458717e-02\n",
      "   7.37025738e-02  -1.43555373e-01  -5.19696139e-02   2.86740869e-01\n",
      "   1.61928743e-01  -1.94252685e-01   4.06761803e-02  -1.71135906e-02\n",
      "   2.61335939e-01   2.47653902e-01  -1.21164620e-01  -1.63612410e-01\n",
      "   2.07370147e-01  -1.25805125e-01   1.28799742e-02  -6.89925402e-02\n",
      "   1.69991374e-01  -1.27530217e-01  -2.29244813e-01  -4.46407944e-02\n",
      "  -3.02916735e-01  -5.61213959e-03  -1.22318633e-01  -2.39220649e-01\n",
      "  -2.40468889e-01   2.11073339e-01  -2.59625465e-02  -2.15992630e-01\n",
      "  -1.44076779e-01  -5.48605733e-02  -1.23214049e-04  -8.35618284e-03\n",
      "  -1.53836638e-01   1.54392272e-01   2.49799758e-01  -3.84318620e-01\n",
      "   3.70111138e-01  -2.73747027e-01   1.74534455e-01  -7.11279288e-02\n",
      "   2.02041745e-01   1.84919965e-02  -1.15326241e-01   7.37414509e-02\n",
      "  -4.12663758e-01  -4.42512482e-02   1.86757728e-01  -4.55847159e-02\n",
      "  -1.25441805e-01  -1.70654953e-01  -2.97856957e-01  -2.27671966e-01\n",
      "   1.44223869e-01   2.67200857e-01   7.63267204e-02  -2.85894275e-01\n",
      "   3.76753360e-01  -4.21232194e-01  -7.59336799e-02  -9.01772082e-02\n",
      "  -6.15902536e-05   2.03948259e-01  -1.41278058e-01  -6.09732568e-02\n",
      "  -5.30279502e-02   8.71461555e-02   9.78587344e-02   1.82244152e-01\n",
      "  -5.22157177e-02   2.56750613e-01   2.29151025e-02   7.39233047e-02\n",
      "   9.00968611e-02   5.64060323e-02  -7.11088777e-02   3.63836557e-01\n",
      "   3.92050855e-02  -3.10258329e-01  -1.06033264e-02   9.25128385e-02\n",
      "   6.90622851e-02   1.40876263e-01  -2.50028789e-01   3.64207685e-01\n",
      "  -2.17658252e-01   8.74292776e-02  -8.69814306e-03   1.14948489e-01\n",
      "  -2.55885720e-02  -1.40407458e-01  -1.69768050e-01   3.16479951e-01\n",
      "   5.85818142e-02  -4.43990305e-02  -9.42739770e-02  -2.51228303e-01\n",
      "  -6.98553771e-02   2.85343289e-01  -1.71919644e-01  -1.31391644e-01\n",
      "  -1.04758054e-01  -8.26577097e-02  -1.54548034e-01  -1.51059881e-01\n",
      "   1.19370699e-01  -1.39911577e-01   2.07959637e-01   9.36279446e-02\n",
      "   7.30661973e-02  -2.11671859e-01  -8.16562772e-02   3.37225311e-02\n",
      "   2.74640173e-01  -1.38880953e-01  -1.45298243e-01   2.23490626e-01\n",
      "   1.25248030e-01   1.31126076e-01   1.06223620e-01  -1.97338283e-01\n",
      "   2.80950397e-01   4.35292184e-01   2.09771432e-02   2.82018125e-01\n",
      "   1.58303395e-01  -1.38046131e-01  -7.02460622e-03  -9.81019139e-02\n",
      "   1.74792573e-01   2.87427511e-02  -8.96195397e-02  -1.35510460e-01\n",
      "   3.06382347e-02   4.59744297e-02  -1.20164797e-01   1.15397036e-01\n",
      "   1.99752986e-01   3.11207205e-01  -1.53450519e-01   1.73042804e-01\n",
      "   1.78634539e-01  -5.61097264e-03  -1.93638936e-01   4.46702866e-03\n",
      "   9.71991420e-02  -5.23704767e-01   3.08654487e-01  -9.40856487e-02\n",
      "   2.83966839e-01   7.42034614e-02  -9.22000688e-03   1.88761324e-01\n",
      "   1.42268538e-01  -1.30276203e-01   1.90095752e-01   7.22765923e-02\n",
      "  -1.46343559e-01   3.28339972e-02  -3.05115074e-01   2.10046366e-01\n",
      "   4.72827047e-01   9.66540277e-02   7.88486972e-02   2.68612150e-03\n",
      "  -4.24888015e-01  -2.28771165e-01  -9.89111513e-02  -1.08684525e-01\n",
      "  -2.03738585e-02   1.81653649e-01   1.84929356e-01   2.20255569e-01\n",
      "  -2.39858285e-01   9.18627232e-02  -1.58822328e-01  -3.89888436e-01\n",
      "  -1.84541449e-01  -6.68990659e-03  -1.60881922e-01  -1.04977116e-01\n",
      "  -9.08884630e-02   1.30077168e-01   2.74325628e-02  -6.17941320e-02\n",
      "   2.28849873e-01   3.54819119e-01   1.14933968e-01   3.62177134e-01\n",
      "   2.29008391e-01  -3.02382588e-01   3.77942696e-02   4.93750364e-01\n",
      "   7.37512559e-02  -5.12147248e-02  -5.25354266e-01  -2.44649053e-01\n",
      "  -1.81735992e-01   2.60092970e-02   4.58716899e-01   1.10289097e-01\n",
      "   7.58599713e-02   3.95208746e-01   7.03118742e-02   1.07789256e-01\n",
      "  -8.48089010e-02  -3.34946036e-01  -1.13417827e-01   3.12381703e-02\n",
      "  -2.20493004e-01   1.27580106e-01   2.11134464e-01   1.18841819e-01\n",
      "  -1.84184864e-01  -3.06580454e-01  -1.57615140e-01   1.14033520e-02\n",
      "  -3.76212269e-01   3.66450548e-02  -3.90500383e-04   1.29537225e-01\n",
      "   2.09503040e-01  -2.77079105e-01   2.41856307e-01   1.51177883e-01\n",
      "  -1.12220705e-01  -3.42347026e-01  -6.51380494e-02  -1.82768226e-01\n",
      "   6.09685741e-02   2.44061008e-01   5.16301394e-02   1.17516622e-03\n",
      "   3.57336909e-01   1.38446659e-01  -1.45189449e-01   1.41101018e-01\n",
      "  -2.38464028e-01   9.41528287e-03  -9.08870548e-02  -1.01453871e-01\n",
      "  -2.04449724e-02  -1.30558640e-01  -1.04156379e-02   2.95672063e-02\n",
      "  -4.56868678e-01   3.26400399e-01  -2.21830934e-01  -4.92064133e-02\n",
      "  -5.13821803e-02  -2.94563085e-01  -1.49099410e-01  -2.98140049e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(train_clean_data[1])\n",
    "print(train_data[1])\n",
    "print(model[u'绿茶'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "lr = 1e-3\n",
    "# 在训练和测试的时候，我们想用不同的 batch_size.所以采用占位符的方式\n",
    "batch_size = tf.placeholder(tf.int32,[])  # 注意类型必须为 tf.int32\n",
    "# 在 1.0 版本以后请使用 ：\n",
    "# keep_prob = tf.placeholder(tf.float32, [])\n",
    "# batch_size = tf.placeholder(tf.int32, [])\n",
    "\n",
    "# 每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素\n",
    "input_size = 324\n",
    "# 时序持续长度为28，即每做一次预测，需要先输入28行\n",
    "timestep_size = 1\n",
    "# 每个隐含层的节点数\n",
    "hidden_size = 324\n",
    "# LSTM layer 的层数\n",
    "layer_num = 2\n",
    "# 最后输出分类类别数量，如果是回归预测的话应该是 1\n",
    "class_num = 2\n",
    "\n",
    "_X = tf.placeholder(tf.float32, [None, input_size])\n",
    "y = tf.placeholder(tf.float32, [None,class_num])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把784个点的字符信息还原成 28 * 28 的图片\n",
    "# 下面几个步骤是实现 RNN / LSTM 的关键\n",
    "####################################################################\n",
    "# **步骤1：RNN 的输入shape = (batch_size, timestep_size, input_size) \n",
    "X = tf.reshape(_X, [-1, timestep_size,input_size])\n",
    "\n",
    "# **步骤2：定义一层 LSTM_cell，只需要说明 hidden_size, 它会自动匹配输入的 X 的维度\n",
    "lstm_cell = rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "# **步骤3：添加 dropout layer, 一般只设置 output_keep_prob\n",
    "lstm_cell = rnn.DropoutWrapper(cell=lstm_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "\n",
    "# **步骤4：调用 MultiRNNCell 来实现多层 LSTM\n",
    "mlstm_cell = rnn.MultiRNNCell([lstm_cell] * layer_num, state_is_tuple=True)\n",
    "\n",
    "# **步骤5：用全零来初始化state\n",
    "init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# **步骤6：方法一，调用 dynamic_rnn() 来让我们构建好的网络运行起来\n",
    "# ** 当 time_major==False 时， outputs.shape = [batch_size, timestep_size, hidden_size] \n",
    "# ** 所以，可以取 h_state = outputs[:, -1, :] 作为最后输出\n",
    "# ** state.shape = [layer_num, 2, batch_size, hidden_size], \n",
    "# ** 或者，可以取 h_state = state[-1][1] 作为最后输出\n",
    "# ** 最后输出维度是 [batch_size, hidden_size]\n",
    "outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=X, initial_state=init_state, time_major=False)\n",
    "h_state = outputs[:, -1, :]  # 或者 h_state = state[-1][1]\n",
    "\n",
    "# *************** 为了更好的理解 LSTM 工作原理，我们把上面 步骤6 中的函数自己来实现 ***************\n",
    "# 通过查看文档你会发现， RNNCell 都提供了一个 __call__()函数（见最后附），我们可以用它来展开实现LSTM按时间步迭代。\n",
    "# **步骤6：方法二，按时间步展开计算\n",
    "# outputs = list()\n",
    "# state = init_state\n",
    "# with tf.variable_scope('RNN'):\n",
    "#     for timestep in range(timestep_size):\n",
    "#         if timestep > 0:\n",
    "#             tf.get_variable_scope().reuse_variables()\n",
    "#         # 这里的state保存了每一层 LSTM 的状态\n",
    "#         (cell_output, state) = mlstm_cell(X[:, timestep, :], state)\n",
    "#         outputs.append(cell_output)\n",
    "# h_state = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面 LSTM 部分的输出会是一个 [hidden_size] 的tensor，我们要分类的话，还需要接一个 softmax 层\n",
    "# 首先定义 softmax 的连接权重矩阵和偏置\n",
    "# out_W = tf.placeholder(tf.float32, [hidden_size, class_num], name='out_Weights')\n",
    "# out_bias = tf.placeholder(tf.float32, [class_num], name='out_bias')\n",
    "# 开始训练和测试\n",
    "W = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n",
    "y_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\n",
    "y_pred_cls = tf.argmax(y_pre, 1)\n",
    "\n",
    "# 损失和评估函数\n",
    "cross_entropy = -tf.reduce_mean(y * tf.log(y_pre))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化sess\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore the model...\n",
      "INFO:tensorflow:Restoring parameters from /notebooks/SpamTrain/model_rnn_03/comment_rnn_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "num_iterations = len(train_clean_data) - 128   \n",
    "\n",
    "if(os.path.exists(\"/notebooks/SpamTrain/model_rnn_03/checkpoint\") == False): \n",
    "    print(\"begin to train...\")\n",
    "    \n",
    "    #处理标签数据\n",
    "    label = train[\"flag\"]\n",
    "    train_data = train_clean_data[:,:]\n",
    "    train_label = label[:]\n",
    "    \n",
    "    # 使用ont-hot编码\n",
    "    enc = OneHotEncoder().fit(train_label.reshape(-1, 1))\n",
    "    train_label = enc.transform(train_label.reshape(-1, 1)).toarray()\n",
    "\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        _batch_size = 128\n",
    "        \n",
    "        offset = (i * _batch_size) % (int(len(train_clean_data) * 1) - _batch_size)\n",
    "\n",
    "        batch_x = train_data[offset:(offset + _batch_size),:]\n",
    "        batch_y = train_label[offset:(offset + _batch_size),:]\n",
    "\n",
    "        if (i+1)%100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={\n",
    "                _X:batch_x, y: batch_y, keep_prob: 1.0, batch_size: _batch_size})\n",
    "            # 已经迭代完成的 epoch 数: mnist.train.epochs_completed\n",
    "            print \" step %d, training accuracy %g\" % ((i+1), train_accuracy)\n",
    "        sess.run(train_op, feed_dict={_X: batch_x, y: batch_y, keep_prob: 0.5, batch_size: _batch_size})\n",
    "        \n",
    "    saver_path = saver.save(sess, \"/notebooks/SpamTrain/model_rnn_03/comment_rnn_model.ckpt\")  # 保存模型\n",
    "else:\n",
    "   print(\"restore the model...\")\n",
    "   saver.restore(sess,'/notebooks/SpamTrain/model_rnn_03/comment_rnn_model.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 324)\n",
      "(8019, 324)\n",
      "[[-0.05196826  0.62578809  0.33745331 ...,  0.07365704 -0.18128303\n",
      "   0.05877255]\n",
      " [-0.13448544 -0.15526298  0.27640295 ..., -0.28622359 -0.1406382\n",
      "  -0.25426164]\n",
      " [-0.04657002  0.63925362 -0.04474411 ..., -0.41634336 -0.48093891\n",
      "   0.03007146]\n",
      " ..., \n",
      " [-0.45400429 -0.66608202  0.32898128 ..., -0.7339204   0.10969409\n",
      "  -0.62974346]\n",
      " [-0.09312254  0.05677414  0.81271517 ..., -0.12547359 -0.39977166\n",
      "   0.511648  ]\n",
      " [-0.52973425  0.04141427  0.20068628 ..., -0.30700812 -0.38732147\n",
      "   0.33269024]]\n",
      "趟 单位 送 钥匙 加微 信聊\n",
      "男主角 大戏 精在 折腾 够呛 平淡 适合 实在 学生妹 法眼 没什么 地方 搭调\n",
      "回复 aa1180 历史 课本上 读到 东西 真 理 这头 蠢货 真的 屠宰场 日本 鬼 子 李向阳 跑 解放 战 争中 苏联 缴获 日本 武器 我军 头脑 太 简单 胜利 想 太 笫 一应 补脑 应 补钙 软骨 狗\n",
      "手 残挡\n",
      "优秀 子 非 同学 昨天 说 发 请 名字 呼唤 链接 推荐 公众 号 资源 很全\n",
      "嗨 解决 咨询 教 提升 能力 执行力 构建 思维 体系 相处 提高 情商 提升 沟通 水平 吸引 异性 获取 异性 亲睐 面对 恋爱 关系 克服 心理障碍 免费 答疑 解惑 恋爱 心态 社交 障碍 人际交往 情商 思维 做事 方法 免费 加群 学习 资料 请加 QQ380125457 希望 成长\n",
      "红外线 针孔 摄像头\n",
      "绿色 生态园 一点 包括 同伴 间 绿色 和谐 关系 潇潇 老师 有意识 幼儿园 发展 理念 落实 幼儿 日常 习惯 培养 正面 角度 引导 幼幼间 绿色 关系 建立 孩子 绿色 氛围 生活 阳光 快乐\n",
      "莪点 飞向 床\n",
      "断断续续 半年 时间 听 完 感触 颇深 与其说是 一部 小说 一种 面对 困境 指导思想 方法论 也许 说 恰当 书中 前置条件 常人 拥有 每次 听到 曾毅 身处 困境 会先 思考 换 听 完 本书 日常 工作 生活 中 处事 方式 改变 书中 观点 工作 之中 听 完 本书 深切 感受 中医 医案 中医 不逊于 西医 学习 中医 知识 用于 日常 养生 保健 一种 谈资 佩服 本书 作者 医好 母亲 疑难 症 创作 令人深思 作品\n",
      "针孔 摄像头\n",
      "配音 心情 订购\n",
      "上海 工作 特么 冷\n",
      "运行 工具 系统 辅助 站外 推广 独立 站 建设\n",
      "公元前 2000 多年 安全套 古埃及 生活 中 称做 阴茎 套 功能 防范 疾病 避孕 女性 佩戴 首饰 当作 装饰品 男人 挂 身上 财富 地位 象征\n",
      "韩东 提到 恩人 夏小 妖儿 真的 找 男人 脚趾头 折腾 夏耀井 中 救人 袁纵 救 妖儿 情节 韩东 手枪 情节 盛势 里 提到\n",
      "听到 毛泽东 时代 男人\n",
      "生命 树 家庭教育\n",
      "假钱 忽悠 司机 太穷\n",
      "身 心灵\n",
      "长白山 香甜 粘 玉米 营养 美味 香甜 适口 送 家人 送 朋友 送 健康 时尚 全国 发售 联系电话 15981305799 微信 同步 诚邀 伙伴\n",
      "张老师 你好 喜欢 节目 节目 很难 理解 艺术 东西 谢谢 2018 老师 讲讲 艺术 先锋 行为艺术 阿布拉莫维奇 古典艺术 至少 能看懂 艺术 实在 看不懂 感觉 中国 古典艺术 喜爱 艺术 太少 张老师 讲\n",
      "淘宝 卖家 大海 老师 音频 听 中肯 思维 方式 内容 受益匪浅 几个 请教 大海 老师 卖家 头疼 中 差评 规避 解决 直通车 刷 听 提起 评价 淘宝 客 推广 希望 每次 音频 时间 长 一点 听到 内容 介绍 做 店 新手 参考 学习 韩都 衣舍 成功 店铺 案例 希望 分享\n",
      "有点像 年代 电视剧 主题曲\n",
      "微信群 进\n",
      "克拉玛依 黑油 山\n",
      "吃 抗生素 人体 副作用 破坏 机体 免疫力 抵抗力 人患 霉菌性 阴道炎 体内 菌 群 破坏 反反复复 很难 康复 建议 采用 中药 配方 产品 治疗\n",
      "订购 放\n",
      "1821810xhdi 合作 呦 手里 好多 资源 介绍 回扣 合作愉快 先 转个 保证金 1000 块 qq862786698\n",
      "佳期 前 几天 下载 测试软件 发现 轻微 自闭症 加 轻微 社交 恐惧症 mmp 这是 注孤生 节奏 想 问问 拿到 36D 鼠标垫 想 陪伴 下半僧 ಥ ಥ\n",
      "淘宝 收件人 写 饕餮 快递 小哥 尴尬 换\n",
      "万晓 推荐 pia 戏 是因为 万晓 喜欢 pia 戏 是因为 留在 荔枝\n",
      "背诵 颐和园 喜欢 段落\n",
      "笃定 爱 宿命 心灵 将会 体验 真实 永恒 生命 荣耀 圣灵 主中 安慰 抚育 只会 信心 满怀 协助 弟兄们 回到 天家 感到 疲惫 有如 翼 之足 冷漠 热情 灵魂深处 批判 一颗 慈爱 之心 只会 表达 真 爱 灵性 发声 害怕 与你同在 天堂 中 深受 惦记 爱 未曾 遗忘 一人\n",
      "叫床 那种\n",
      "回忆起 十年 前 重庆 出发 自驾 南北 缰 出头 回来 洗 几百张 照片 每到 地方 网络 笔记本电脑 qq 空间 同步 发些 当天 照片 刚刚 岁 女儿 自驾 新西兰 南北 岛 一个月 驱车 一共 4000 多公里 不算 跨 库克 海峡 渡轮 景点 要票 坐 统一 大巴 霍比特 屯 私人 领地 爱 去不去 鑫 缰 海 内陆 好美 喀纳斯 巴音 天鹅湖 几只 鹅 新西兰 天鹅 成灾 零距离 接触 抓紧 抱 合影 一点 夸大 新西兰 河流 湖泊 众多 住 湖边 挺 主播 记录 写点 东西 谢谢 回忆 辽阔 鑫 缰\n",
      "读 课文 玩意儿 捧 chou 脚 还特 好意思 收费 次奥\n",
      "孩子 过敏 长 湿疹 便秘 松花粉 无限极\n",
      "仿 东京 食尸 鬼 至少 设定\n",
      "女主 真的 弱智 重生 害过 善良 不去 铲除 起码 防范 意识\n",
      "放寒假 初二 女孩 电子产品 不离手 不想 学习 爸爸 画画 说 朋友 不去\n",
      "融 认购 宽带接入 女 富豪 喝点 微博 介绍 习近平 不行 万科 巨额 经纪人 活动 火热 喝 机顶盒 好好 好好 whov 武汉队 社会 导火索 额 济南市 计算机 安静 二季度 恢复 IEIE 焦恩俊 金融界 人口 贷款 人口 坑人 伏明霞 时间 度假 酒店 金额 金额 鸡尾酒 胃口 焦恩俊 王家卫 今生今世 计算机 思想家 只能 高福鹏 回到 家 EVW 新安江 病毒库 乔布斯 狂犬病 自强 v 年轻 说不上\n",
      "(8019, 324)\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "batch_x = train_clean_data[0:1000,:]\n",
    "print(batch_x.shape)\n",
    "print(train_clean_data.shape)\n",
    "pre = sess.run(y_pred_cls, {_X:batch_x, keep_prob: 1.0,batch_size: 1000})\n",
    "\n",
    "# print(type(train_clean_data))\n",
    "# print(train_clean_data)\n",
    "# print(2053976 / 256)\n",
    "print(batch_x)\n",
    "\n",
    "nwords = 0\n",
    "for i in range(len(pre)):\n",
    "    if(pre[i] == 1):\n",
    "        print(train_data[i])\n",
    "#         train_clean_data = np.append(train_clean_data,train_clean_data[i])\n",
    "        nwords += 1\n",
    "\n",
    "# train_clean_data = train_clean_data.reshape(-1,2053976 / 256,256)\n",
    "print(train_clean_data.shape)\n",
    "# print(train_clean_data.shape[0] / 256)\n",
    "# print(train_clean_data)\n",
    "\n",
    "print(nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
